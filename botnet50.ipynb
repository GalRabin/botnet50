{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJxaHtQDuHrSij0nmyBZtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GalRabin/botnet50/blob/main/botnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvAIM9AI2YLN"
      },
      "source": [
        "# Bottleneck Transformers for Visual Recognition (Link to BotNet50 [Article](https://arxiv.org/pdf/2101.11605v1.pdf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-OCwJ3a2n_2"
      },
      "source": [
        "## BotNet50 implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLRhtQ0-110U"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6EeTlVNz8aB",
        "outputId": "c4e88092-7e52-4947-ee1a-ab44e8dfd4b4"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.6/dist-packages (0.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buipsx-21f_7"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPGTHH5GwN-7"
      },
      "source": [
        "###################\n",
        "##### Imports #####\n",
        "###################\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "\n",
        "from einops import rearrange\n",
        "from torchvision.models import resnet50"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-A3McBk1exE"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjmtCP1rzm-U"
      },
      "source": [
        "def pair(x):\n",
        "    return (x, x) if not isinstance(x, tuple) else x\n",
        "\n",
        "\n",
        "def expand_dim(t, dim, k):\n",
        "    t = t.unsqueeze(dim=dim)\n",
        "    expand_shape = [-1] * len(t.shape)\n",
        "    expand_shape[dim] = k\n",
        "    return t.expand(*expand_shape)\n",
        "\n",
        "\n",
        "def rel_to_abs(x):\n",
        "    b, h, l, _, device, dtype = *x.shape, x.device, x.dtype\n",
        "    dd = {'device': device, 'dtype': dtype}\n",
        "    col_pad = torch.zeros((b, h, l, 1), **dd)\n",
        "    x = torch.cat((x, col_pad), dim=3)\n",
        "    flat_x = rearrange(x, 'b h l c -> b h (l c)')\n",
        "    flat_pad = torch.zeros((b, h, l - 1), **dd)\n",
        "    flat_x_padded = torch.cat((flat_x, flat_pad), dim=2)\n",
        "    final_x = flat_x_padded.reshape(b, h, l + 1, 2 * l - 1)\n",
        "    final_x = final_x[:, :, :l, (l - 1):]\n",
        "    return final_x\n",
        "\n",
        "\n",
        "def relative_logits_1d(q, rel_k):\n",
        "    b, heads, h, w, dim = q.shape\n",
        "    logits = einsum('b h x y d, r d -> b h x y r', q, rel_k)\n",
        "    logits = rearrange(logits, 'b h x y r -> b (h x) y r')\n",
        "    logits = rel_to_abs(logits)\n",
        "    logits = logits.reshape(b, heads, h, w, w)\n",
        "    logits = expand_dim(logits, dim=3, k=h)\n",
        "    return logits\n",
        "\n",
        "\n",
        "# positional embeddings\n",
        "\n",
        "class AbsPosEmb(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            fmap_size,\n",
        "            dim_head\n",
        "    ):\n",
        "        super().__init__()\n",
        "        height, width = pair(fmap_size)\n",
        "        scale = dim_head ** -0.5\n",
        "        self.height = nn.Parameter(torch.randn(height, dim_head) * scale)\n",
        "        self.width = nn.Parameter(torch.randn(width, dim_head) * scale)\n",
        "\n",
        "    def forward(self, q):\n",
        "        emb = rearrange(self.height, 'h d -> h () d') + rearrange(self.width, 'w d -> () w d')\n",
        "        emb = rearrange(emb, ' h w d -> (h w) d')\n",
        "        logits = einsum('b h i d, j d -> b h i j', q, emb)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class RelPosEmb(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            fmap_size,\n",
        "            dim_head\n",
        "    ):\n",
        "        super().__init__()\n",
        "        height, width = pair(fmap_size)\n",
        "        scale = dim_head ** -0.5\n",
        "        self.fmap_size = fmap_size\n",
        "        self.rel_height = nn.Parameter(torch.randn(height * 2 - 1, dim_head) * scale)\n",
        "        self.rel_width = nn.Parameter(torch.randn(width * 2 - 1, dim_head) * scale)\n",
        "\n",
        "    def forward(self, q):\n",
        "        h, w = self.fmap_size\n",
        "\n",
        "        q = rearrange(q, 'b h (x y) d -> b h x y d', x=h, y=w)\n",
        "        rel_logits_w = relative_logits_1d(q, self.rel_width)\n",
        "        rel_logits_w = rearrange(rel_logits_w, 'b h x i y j-> b h (x y) (i j)')\n",
        "\n",
        "        q = rearrange(q, 'b h x y d -> b h y x d')\n",
        "        rel_logits_h = relative_logits_1d(q, self.rel_height)\n",
        "        rel_logits_h = rearrange(rel_logits_h, 'b h x i y j -> b h (y x) (j i)')\n",
        "        return rel_logits_w + rel_logits_h"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UvDmKkx1r74"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXCVVceI0Q6J"
      },
      "source": [
        "#####################\n",
        "##### Attention #####\n",
        "#####################\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            *,\n",
        "            dim,\n",
        "            fmap_size,\n",
        "            heads=4,\n",
        "            dim_head=128,\n",
        "            rel_pos_emb=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        inner_dim = heads * dim_head\n",
        "\n",
        "        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, 1, bias=False)\n",
        "\n",
        "        rel_pos_class = AbsPosEmb if not rel_pos_emb else RelPosEmb\n",
        "        self.pos_emb = rel_pos_class(fmap_size, dim_head)\n",
        "\n",
        "    def forward(self, fmap):\n",
        "        heads, b, c, h, w = self.heads, *fmap.shape\n",
        "\n",
        "        q, k, v = self.to_qkv(fmap).chunk(3, dim=1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> b h (x y) d', h=heads), (q, k, v))\n",
        "\n",
        "        q *= self.scale\n",
        "\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        sim += self.pos_emb(q)\n",
        "\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=h, y=w)\n",
        "        return out"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Hl7DRc2Ivz"
      },
      "source": [
        "### BottleBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxoeIHAb0auZ"
      },
      "source": [
        "class BottleBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            *,\n",
        "            dim,\n",
        "            fmap_size,\n",
        "            dim_out,\n",
        "            proj_factor,\n",
        "            downsample,\n",
        "            heads=4,\n",
        "            dim_head=128,\n",
        "            rel_pos_emb=False,\n",
        "            activation=nn.ReLU()\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # shortcut\n",
        "\n",
        "        if dim != dim_out or downsample:\n",
        "            kernel_size, stride, padding = (3, 2, 1) if downsample else (1, 1, 0)\n",
        "\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(dim, dim_out, kernel_size, stride=stride, padding=padding, bias=False),\n",
        "                nn.BatchNorm2d(dim_out),\n",
        "                activation\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        # contraction and expansion\n",
        "\n",
        "        attn_dim_in = dim_out // proj_factor\n",
        "        attn_dim_out = heads * dim_head\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim, attn_dim_in, 1, bias=False),\n",
        "            nn.BatchNorm2d(attn_dim_in),\n",
        "            activation,\n",
        "            Attention(\n",
        "                dim=attn_dim_in,\n",
        "                fmap_size=fmap_size,\n",
        "                heads=heads,\n",
        "                dim_head=dim_head,\n",
        "                rel_pos_emb=rel_pos_emb\n",
        "            ),\n",
        "            nn.AvgPool2d((2, 2)) if downsample else nn.Identity(),\n",
        "            nn.BatchNorm2d(attn_dim_out),\n",
        "            activation,\n",
        "            nn.Conv2d(attn_dim_out, dim_out, 1, bias=False),\n",
        "            nn.BatchNorm2d(dim_out)\n",
        "        )\n",
        "\n",
        "        # init last batch norm gamma to zero\n",
        "\n",
        "        nn.init.zeros_(self.net[-1].weight)\n",
        "\n",
        "        # final activation\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = self.net(x)\n",
        "        x += shortcut\n",
        "        return self.activation(x)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49xWJZUi2AJa"
      },
      "source": [
        "### BottleStack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOg-Kw1r0lAG"
      },
      "source": [
        "class BottleStack(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            *,\n",
        "            dim,\n",
        "            fmap_size,\n",
        "            dim_out=2048,\n",
        "            proj_factor=4,\n",
        "            num_layers=3,\n",
        "            heads=4,\n",
        "            dim_head=128,\n",
        "            downsample=True,\n",
        "            rel_pos_emb=False,\n",
        "            activation=nn.ReLU()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        fmap_size = pair(fmap_size)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.fmap_size = fmap_size\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            is_first = i == 0\n",
        "            dim = (dim if is_first else dim_out)\n",
        "            layer_downsample = is_first and downsample\n",
        "\n",
        "            fmap_divisor = (2 if downsample and not is_first else 1)\n",
        "            layer_fmap_size = tuple(map(lambda t: t // fmap_divisor, fmap_size))\n",
        "\n",
        "            layers.append(BottleBlock(\n",
        "                dim=dim,\n",
        "                fmap_size=layer_fmap_size,\n",
        "                dim_out=dim_out,\n",
        "                proj_factor=proj_factor,\n",
        "                heads=heads,\n",
        "                dim_head=dim_head,\n",
        "                downsample=layer_downsample,\n",
        "                rel_pos_emb=rel_pos_emb,\n",
        "                activation=activation\n",
        "            ))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, c, h, w = x.shape\n",
        "        assert c == self.dim, f'channels of feature map {c} must match channels given at init {self.dim}'\n",
        "        assert h == self.fmap_size[0] and w == self.fmap_size[\n",
        "            1], f'height and width ({h} {w}) of feature map must match the fmap_size given at init {self.fmap_size}'\n",
        "        return self.net(x)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu8yYZJR2QPv"
      },
      "source": [
        "### BotNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp33y5Lo0uu1"
      },
      "source": [
        "class BotNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BotNet50, self).__init__()\n",
        "        layer = BottleStack(\n",
        "            dim=256,\n",
        "            fmap_size=56,  # set specifically for imagenet's 224 x 224\n",
        "            dim_out=2048,\n",
        "            proj_factor=4,\n",
        "            downsample=True,\n",
        "            heads=4,\n",
        "            dim_head=128,\n",
        "            rel_pos_emb=True,\n",
        "            activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "        resnet = resnet50()\n",
        "\n",
        "        # model surgery\n",
        "\n",
        "        backbone = list(resnet.children())\n",
        "\n",
        "        self._model = nn.Sequential(\n",
        "            *backbone[:7],\n",
        "            layer,\n",
        "            *backbone[8:],\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self._model(img)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rnlk_ni3BNP"
      },
      "source": [
        "## BotNet50 train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkSOtIW33HZA"
      },
      "source": [
        "## BotNet50 test"
      ]
    }
  ]
}